{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f26d7f2-f9d9-4862-a1b4-0354e4dfddbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>product_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>B09XWYG6X1</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>B0BXDLF8TW</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>B09G2PW8ZG</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>B08CSZDXZY</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>B0C6V27S6N</td>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  product_id                       user_id\n",
       "0     1.0  B09XWYG6X1  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ\n",
       "1     5.0  B0BXDLF8TW  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ\n",
       "2     2.0  B09G2PW8ZG  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ\n",
       "3     5.0  B08CSZDXZY  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ\n",
       "4     5.0  B0C6V27S6N  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read a single Parquet chunk from your user ratings dataset\n",
    "df_user_ratings = pd.read_parquet(\n",
    "    \"gs://recomviz_home_and_kitchen/datasets/converted_user_ratings/chunk_00000.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"token\": \"cloud\"}  # Required in Vertex AI\n",
    ")\n",
    "\n",
    "# Preview it\n",
    "df_user_ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebe67af-9475-4983-886b-dcde67eb0f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Set of 4 Irish Coffee Glass Mugs Footed 10.5 o...</td>\n",
       "      <td>B07R3DYMH6</td>\n",
       "      <td>[Set of 12 Footed 10.5 oz. Irish coffee mug th...</td>\n",
       "      <td>{'hi_res': ['https://m.media-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Foaming Soap Dispenser Thick Ceramic Foam Hand...</td>\n",
       "      <td>B0BNZ8Q7YT</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'hi_res': ['https://m.media-amazon.com/images...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tapestry Trading 558W90 90 in. European Lace T...</td>\n",
       "      <td>B01508WQC6</td>\n",
       "      <td>[Features. European Lace Tablecloth. 100 Polye...</td>\n",
       "      <td>{'hi_res': [None], 'large': ['https://m.media-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jersey seating 2 x Vinyl Air Lift Adjustable S...</td>\n",
       "      <td>B00KKU8HTG</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'hi_res': [None, 'https://m.media-amazon.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chisander 20 Inches Grey with White Super Soft...</td>\n",
       "      <td>B0B61RJ848</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'hi_res': ['https://m.media-amazon.com/images...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  product_id  \\\n",
       "0  Set of 4 Irish Coffee Glass Mugs Footed 10.5 o...  B07R3DYMH6   \n",
       "1  Foaming Soap Dispenser Thick Ceramic Foam Hand...  B0BNZ8Q7YT   \n",
       "2  Tapestry Trading 558W90 90 in. European Lace T...  B01508WQC6   \n",
       "3  jersey seating 2 x Vinyl Air Lift Adjustable S...  B00KKU8HTG   \n",
       "4  Chisander 20 Inches Grey with White Super Soft...  B0B61RJ848   \n",
       "\n",
       "                                         description  \\\n",
       "0  [Set of 12 Footed 10.5 oz. Irish coffee mug th...   \n",
       "1                                                 []   \n",
       "2  [Features. European Lace Tablecloth. 100 Polye...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                              images  \n",
       "0  {'hi_res': ['https://m.media-amazon.com/images...  \n",
       "1  {'hi_res': ['https://m.media-amazon.com/images...  \n",
       "2  {'hi_res': [None], 'large': ['https://m.media-...  \n",
       "3  {'hi_res': [None, 'https://m.media-amazon.com/...  \n",
       "4  {'hi_res': ['https://m.media-amazon.com/images...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_item_metadata = pd.read_parquet(\n",
    "    \"gs://recomviz_home_and_kitchen/datasets/converted_item_metadata/chunk_00000.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"token\": \"cloud\"}\n",
    ")\n",
    "\n",
    "df_item_metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0578a7d2-c7d9-442e-a83a-1b1438ecf82a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,000,000 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#######################################\n",
    "# Create sample from user ratings data\n",
    "#######################################\n",
    "\n",
    "# List of chunks to load for user ratings (adjust as needed)\n",
    "chunk_paths = [\n",
    "    f\"gs://recomviz_home_and_kitchen/datasets/converted_user_ratings/chunk_{i:05}.parquet\"\n",
    "    for i in range(10)  # adjust this range to control size (e.g., 0–10 chunks)\n",
    "]\n",
    "\n",
    "# Load and concatenate user ratings chunks\n",
    "dfs = [\n",
    "    pd.read_parquet(path, engine=\"pyarrow\", storage_options={\"token\": \"cloud\"})\n",
    "    for path in chunk_paths\n",
    "]\n",
    "\n",
    "full_df_ratings = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(full_df_ratings):,} rows of user ratings\")\n",
    "\n",
    "# Create a sample of 100,000 rows from the user ratings data\n",
    "sample_ratings_df = full_df_ratings.sample(n=100_000, random_state=42)  # adjust as needed\n",
    "sample_ratings_df.to_parquet(\"large_sample_user_ratings.parquet\", index=False)\n",
    "\n",
    "# Extract unique product_ids from the user ratings sample\n",
    "unique_product_ids = sample_ratings_df['product_id'].unique()\n",
    "print(f\"Number of unique product_ids in sample: {len(unique_product_ids):,}\")\n",
    "\n",
    "###########################################################\n",
    "# Create a consistent sample from item metadata data\n",
    "###########################################################\n",
    "\n",
    "# List of chunks to load for item metadata (adjust as needed)\n",
    "chunk_paths = [\n",
    "    f\"gs://recomviz_home_and_kitchen/datasets/converted_item_metadata/chunk_{i:05}.parquet\"\n",
    "    for i in range(10)  # adjust this range to control size (e.g., 0–10 chunks)\n",
    "]\n",
    "\n",
    "# Load and concatenate item metadata chunks\n",
    "dfs = [\n",
    "    pd.read_parquet(path, engine=\"pyarrow\", storage_options={\"token\": \"cloud\"})\n",
    "    for path in chunk_paths\n",
    "]\n",
    "\n",
    "full_df_item = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(full_df_item):,} rows of item metadata\")\n",
    "\n",
    "# Filter item metadata so that only items found in the user ratings sample are kept\n",
    "filtered_item_df = full_df_item[full_df_item['product_id'].isin(unique_product_ids)]\n",
    "print(f\"Filtered item metadata contains {len(filtered_item_df):,} rows\")\n",
    "\n",
    "# Optionally, if you want to restrict the sample to a fixed number (e.g., 20,000 rows)\n",
    "if len(filtered_item_df) > 20_000:\n",
    "    sample_item_df = filtered_item_df.sample(n=20_000, random_state=42)  # adjust as needed\n",
    "else:\n",
    "    sample_item_df = filtered_item_df\n",
    "\n",
    "sample_item_df.to_parquet(\"large_sample_item_info.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b111cfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample user ratings: 100,000 rows\n",
      "Unique product_ids in sample: 69,877\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Please install gcsfs to access Google Storage",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fsspec/registry.py:242\u001b[0m, in \u001b[0;36mget_filesystem_class\u001b[0;34m(protocol)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m     register_implementation(protocol, \u001b[43m_import_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbit\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fsspec/registry.py:277\u001b[0m, in \u001b[0;36m_import_class\u001b[0;34m(fqp)\u001b[0m\n\u001b[1;32m    276\u001b[0m is_s3 \u001b[38;5;241m=\u001b[39m mod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3fs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 277\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_s3 \u001b[38;5;129;01mand\u001b[39;00m mod\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gcsfs'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     20\u001b[0m chunk_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://recomviz_home_and_kitchen/datasets/converted_item_metadata/chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# adjust this range (e.g., 0–10 chunks)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Load and concatenate item metadata chunks\u001b[39;00m\n\u001b[1;32m     26\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcloud\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m chunk_paths\n\u001b[1;32m     29\u001b[0m ]\n\u001b[1;32m     30\u001b[0m full_df_item \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dfs, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(full_df_item)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows of item metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parquet.py:122\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m         fsspec \u001b[38;5;241m=\u001b[39m import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfsspec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m         fs, path_or_handle \u001b[38;5;241m=\u001b[39m \u001b[43mfsspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl_to_fs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m storage_options \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_url(path_or_handle) \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# can't write to a remote url\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# without making use of fsspec at the moment\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options passed with buffer, or non-supported URL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fsspec/core.py:396\u001b[0m, in \u001b[0;36murl_to_fs\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m known_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    394\u001b[0m }\n\u001b[1;32m    395\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m known_kwargs}\n\u001b[0;32m--> 396\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43m_un_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m inkwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Reverse iterate the chain, creating a nested target_* structure\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fsspec/core.py:344\u001b[0m, in \u001b[0;36m_un_chain\u001b[0;34m(path, kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(bits):\n\u001b[1;32m    343\u001b[0m     protocol \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m split_protocol(bit)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mget_filesystem_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     extra_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_kwargs_from_urls(bit)\n\u001b[1;32m    346\u001b[0m     kws \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(protocol, {})\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/fsspec/registry.py:244\u001b[0m, in \u001b[0;36mget_filesystem_class\u001b[0;34m(protocol)\u001b[0m\n\u001b[1;32m    242\u001b[0m         register_implementation(protocol, _import_class(bit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(bit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merr\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m registry[protocol]\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotocol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mImportError\u001b[0m: Please install gcsfs to access Google Storage"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#######################################\n",
    "# Use Existing Sample User Ratings Data\n",
    "#######################################\n",
    "\n",
    "# Load the existing sample user ratings file (from your long-run algorithm)\n",
    "sample_ratings_df = pd.read_parquet(\"sample_user_ratings.parquet\")\n",
    "print(f\"Loaded sample user ratings: {len(sample_ratings_df):,} rows\")\n",
    "\n",
    "# Extract unique product IDs from the sample\n",
    "unique_product_ids = sample_ratings_df['product_id'].unique()\n",
    "print(f\"Unique product_ids in sample: {len(unique_product_ids):,}\")\n",
    "\n",
    "######################################################\n",
    "# Create a Consistent Sample from Item Metadata Data\n",
    "######################################################\n",
    "\n",
    "# List of chunks to load for item metadata (adjust the range as needed)\n",
    "chunk_paths = [\n",
    "    f\"gs://recomviz_home_and_kitchen/datasets/converted_item_metadata/chunk_{i:05}.parquet\"\n",
    "    for i in range(10)  # adjust this range (e.g., 0–10 chunks)\n",
    "]\n",
    "\n",
    "# Load and concatenate item metadata chunks\n",
    "dfs = [\n",
    "    pd.read_parquet(path, engine=\"pyarrow\", storage_options={\"token\": \"cloud\"})\n",
    "    for path in chunk_paths\n",
    "]\n",
    "full_df_item = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(full_df_item):,} rows of item metadata\")\n",
    "\n",
    "# Filter item metadata so that only items with product IDs that appear in the sample user ratings are kept\n",
    "filtered_item_df = full_df_item[full_df_item['product_id'].isin(unique_product_ids)]\n",
    "print(f\"Filtered item metadata contains {len(filtered_item_df):,} rows\")\n",
    "\n",
    "# Optionally, if you want to restrict the sample to a fixed number (e.g., 20,000 rows)\n",
    "if len(filtered_item_df) > 20_000:\n",
    "    sample_item_df = filtered_item_df.sample(n=20_000, random_state=42)\n",
    "else:\n",
    "    sample_item_df = filtered_item_df\n",
    "\n",
    "# Save the filtered sample to a new Parquet file\n",
    "sample_item_df.to_parquet(\"updated_sample_item_info.parquet\", index=False)\n",
    "print(\"Saved filtered item info sample as 'updated_sample_item_info.parquet'\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
