{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef05802a-3074-4ee7-ba23-bba48aa761c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c9ee83b5df402cb3cee885b20487cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    full: Dataset({\n",
      "        features: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'images', 'videos', 'store', 'categories', 'details', 'parent_asin', 'bought_together', 'subtitle', 'author'],\n",
      "        num_rows: 3735584\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# %pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Amazon Reviews 2023 dataset\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_Home_and_Kitchen\", trust_remote_code=True)\n",
    "\n",
    "# Print the dataset splits and details\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c855f-8b78-4cf4-bf0b-14a0d1cf6002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"amazon_products_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d8880-101a-44a5-a757-bc0fa5fa9f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set your project and bucket name\n",
    "bucket_name = 'recomviz_home_and_kitchen'\n",
    "destination_folder = 'datasets/amazon_products_dataset'  # GCS path\n",
    "\n",
    "# Initialize storage client\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Upload function\n",
    "def upload_folder_to_gcs(local_path, gcs_path):\n",
    "    for root, _, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_file, local_path)\n",
    "            blob_path = os.path.join(gcs_path, relative_path)\n",
    "            blob = bucket.blob(blob_path)\n",
    "            blob.upload_from_filename(local_file)\n",
    "            print(f\"Uploaded {local_file} to {blob_path}\")\n",
    "\n",
    "# Call the function\n",
    "upload_folder_to_gcs(\"amazon_products_dataset\", destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40affc1-85ee-4612-a30d-0a222c99ea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install google-cloud-storage\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "def download_gcs_folder(bucket_name, gcs_folder, local_folder):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    blobs = bucket.list_blobs(prefix=gcs_folder)\n",
    "\n",
    "    for blob in blobs:\n",
    "        # Get local path\n",
    "        local_path = os.path.join(local_folder, os.path.relpath(blob.name, gcs_folder))\n",
    "\n",
    "        # Create local directories if needed\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "\n",
    "        # Download the file\n",
    "        blob.download_to_filename(local_path)\n",
    "        print(f\"Downloaded {blob.name} to {local_path}\")\n",
    "\n",
    "# 🔁 Replace with your values:\n",
    "bucket_name = \"recomviz_home_and_kitchen\"\n",
    "gcs_folder = \"datasets/amazon_products_dataset\"  # GCS path (folder)\n",
    "local_folder = \"amazon_products_dataset\"          # Local target\n",
    "\n",
    "download_gcs_folder(bucket_name, gcs_folder, local_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1be043a-f916-461f-819a-70175d247185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'parent_asin', 'description', 'images'],\n",
      "    num_rows: 3735584\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "item_item_dataset = dataset[\"full\"].select_columns([\"title\", \"parent_asin\", \"description\", \"images\"])\n",
    "print(item_item_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a715a3-8409-4835-a9c6-373f0ae2175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 100_000\n",
    "total_rows = item_item_dataset.num_rows\n",
    "bucket_path = \"gs://recomviz_home_and_kitchen/datasets/converted_item_metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d462cc6-99d8-47a0-8bb2-697f34facf27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading item metadata in batches: 100%|██████████| 38/38 [03:17<00:00,  5.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# Batch conversion with progress bar\n",
    "for i in tqdm(range(0, total_rows, batch_size), desc=\"Uploading item metadata in batches\"):\n",
    "    # Select batch\n",
    "    batch = item_item_dataset.select(range(i, min(i + batch_size, total_rows)))\n",
    "    df = batch.to_pandas()\n",
    "    \n",
    "    # Drop nulls\n",
    "    df = df.dropna(subset=[\"parent_asin\", \"title\", \"description\"])\n",
    "    \n",
    "    # Rename for consistency\n",
    "    df.rename(columns={\"parent_asin\": \"product_id\"}, inplace=True)\n",
    "    \n",
    "    # Save batch as Parquet to GCS\n",
    "    df.to_parquet(\n",
    "        f\"{bucket_path}/chunk_{i//batch_size:05}.parquet\",\n",
    "        index=False,\n",
    "        engine=\"pyarrow\",\n",
    "        storage_options={\"token\": \"cloud\"}  # Vertex AI-compatible\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad288d4-7047-42b6-babd-c5b6e40f5f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
